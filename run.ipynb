{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "import imp\n",
    "import core\n",
    "import utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd = 'E:/new_data/kaggle/planet/'\n",
    "train_set = pd.read_csv(wd + 'train_v2.csv')\n",
    "train_set['tags'] = train_set['tags'].apply(lambda x: x.split(' '))\n",
    "test_set = pd.read_csv(wd+'sample_submission_v2.csv')\n",
    "train_tags = ['clear', 'partly_cloudy', 'haze', 'cloudy', 'primary', 'agriculture', 'road', 'water',\n",
    "             'cultivation', 'habitation', 'bare_ground', 'selective_logging', 'artisinal_mine', \n",
    "              'blooming', 'slash_burn', 'conventional_mine', 'blow_down']\n",
    "label_map = {l: i for i, l in enumerate(train_tags)}\n",
    "inv_label_map = {i: l for l, i in label_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 载入数据\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "file_all = train_set['image_name'].values\n",
    "y_all = utils.get_y(train_set['tags'].values, label_map)\n",
    "x_tr, x_vl, y_tr, y_vl = train_test_split(file_all, y_all, test_size=0.8, random_state=int(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from PIL import Image\n",
    "from time import time\n",
    "import mydataset\n",
    "imp.reload(utils)\n",
    "imp.reload(core)\n",
    "imp.reload(mydataset)\n",
    "import gc\n",
    "for i in range(10):\n",
    "    gc.collect()\n",
    "\n",
    "# 超参数\n",
    "batch_size = 256\n",
    "pic_size = (64, 64)\n",
    "learning_rate = 1e-3\n",
    "num_epoches = 1000\n",
    "tolerance = 15\n",
    "lr_tolerance = 7\n",
    "best_model = core.BestModel()\n",
    "transform1 = transforms.Compose([\n",
    "#     transforms.RandomCrop(pic_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "degrees = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "rotate_range = (0, 45)\n",
    "def transform_tr(img, pic_size):\n",
    "    img = img.resize(pic_size)\n",
    "    img = img.transpose(Image.FLIP_LEFT_RIGHT)  if random.randint(0, 1) > .5 else img\n",
    "    img = img.transpose(Image.FLIP_TOP_BOTTOM)  if random.randint(0, 1) > .5 else img\n",
    "    img.rotate(np.random.random() *  (rotate_range[1] - rotate_range[0]) + rotate_range[0])\n",
    "    img_tensor = transform2(img)\n",
    "    return img_tensor\n",
    "\n",
    "def transform_vl(img, pic_size):\n",
    "    img = img.resize(pic_size)\n",
    "    img_tensor = transform2(img)\n",
    "    return img_tensor\n",
    "\n",
    "probs1 = [0.1, 0.2, 0.4, 0.4, 0.1, 0.1, 0.2, 0.2, 0.4, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "probs = [0.5, 0.6, 0.8, 0.8, 0.5, 0.5, 0.6, 0.6, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Model!\n",
      "[1/1000] 662s, Loss: 0.6187, Val-Loss: 0.4695, Best Val-Loss: 0.4695, Val-F2: 0.49\n",
      "Update Model!\n",
      "[2/1000] 120s, Loss: 0.3541, Val-Loss: 0.2289, Best Val-Loss: 0.2289, Val-F2: 0.78\n",
      "Update Model!\n",
      "[3/1000] 103s, Loss: 0.2116, Val-Loss: 0.1695, Best Val-Loss: 0.1695, Val-F2: 0.82\n",
      "[4/1000] 97s, Loss: 0.1917, Val-Loss: 0.1792, Best Val-Loss: 0.1695, Val-F2: 0.82\n",
      "Update Model!\n",
      "[5/1000] 95s, Loss: 0.1865, Val-Loss: 0.1678, Best Val-Loss: 0.1678, Val-F2: 0.83\n",
      "Update Model!\n",
      "[6/1000] 94s, Loss: 0.1753, Val-Loss: 0.1475, Best Val-Loss: 0.1475, Val-F2: 0.85\n",
      "[7/1000] 93s, Loss: 0.1761, Val-Loss: 0.1547, Best Val-Loss: 0.1475, Val-F2: 0.84\n",
      "[8/1000] 93s, Loss: 0.1705, Val-Loss: 0.1555, Best Val-Loss: 0.1475, Val-F2: 0.84\n",
      "Update Model!\n",
      "[9/1000] 93s, Loss: 0.1644, Val-Loss: 0.1403, Best Val-Loss: 0.1403, Val-F2: 0.86\n",
      "[10/1000] 93s, Loss: 0.1641, Val-Loss: 0.1416, Best Val-Loss: 0.1403, Val-F2: 0.86\n",
      "[11/1000] 93s, Loss: 0.1596, Val-Loss: 0.1479, Best Val-Loss: 0.1403, Val-F2: 0.86\n",
      "[12/1000] 93s, Loss: 0.1580, Val-Loss: 0.1403, Best Val-Loss: 0.1403, Val-F2: 0.86\n",
      "Update Model!\n",
      "[13/1000] 93s, Loss: 0.1553, Val-Loss: 0.1376, Best Val-Loss: 0.1376, Val-F2: 0.86\n",
      "Update Model!\n",
      "[14/1000] 93s, Loss: 0.1541, Val-Loss: 0.1347, Best Val-Loss: 0.1347, Val-F2: 0.86\n",
      "[15/1000] 93s, Loss: 0.1541, Val-Loss: 0.1414, Best Val-Loss: 0.1347, Val-F2: 0.86\n",
      "[16/1000] 93s, Loss: 0.1508, Val-Loss: 0.1494, Best Val-Loss: 0.1347, Val-F2: 0.85\n",
      "[17/1000] 93s, Loss: 0.1513, Val-Loss: 0.1394, Best Val-Loss: 0.1347, Val-F2: 0.86\n",
      "Update Model!\n",
      "[18/1000] 93s, Loss: 0.1504, Val-Loss: 0.1338, Best Val-Loss: 0.1338, Val-F2: 0.87\n",
      "Update Model!\n",
      "[19/1000] 93s, Loss: 0.1440, Val-Loss: 0.1271, Best Val-Loss: 0.1271, Val-F2: 0.88\n",
      "[20/1000] 93s, Loss: 0.1453, Val-Loss: 0.1331, Best Val-Loss: 0.1271, Val-F2: 0.86\n",
      "[21/1000] 93s, Loss: 0.1421, Val-Loss: 0.1433, Best Val-Loss: 0.1271, Val-F2: 0.87\n",
      "Update Model!\n",
      "[22/1000] 93s, Loss: 0.1448, Val-Loss: 0.1254, Best Val-Loss: 0.1254, Val-F2: 0.88\n",
      "[23/1000] 93s, Loss: 0.1408, Val-Loss: 0.1313, Best Val-Loss: 0.1254, Val-F2: 0.87\n",
      "Update Model!\n",
      "[24/1000] 93s, Loss: 0.1389, Val-Loss: 0.1254, Best Val-Loss: 0.1254, Val-F2: 0.88\n",
      "Update Model!\n",
      "[25/1000] 93s, Loss: 0.1361, Val-Loss: 0.1246, Best Val-Loss: 0.1246, Val-F2: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/1000] 93s, Loss: 0.1415, Val-Loss: 0.1307, Best Val-Loss: 0.1246, Val-F2: 0.87\n",
      "[27/1000] 93s, Loss: 0.1403, Val-Loss: 0.1271, Best Val-Loss: 0.1246, Val-F2: 0.87\n",
      "Update Model!\n",
      "[28/1000] 93s, Loss: 0.1375, Val-Loss: 0.1189, Best Val-Loss: 0.1189, Val-F2: 0.89\n",
      "[29/1000] 93s, Loss: 0.1369, Val-Loss: 0.1211, Best Val-Loss: 0.1189, Val-F2: 0.88\n",
      "Update Model!\n",
      "[30/1000] 93s, Loss: 0.1314, Val-Loss: 0.1185, Best Val-Loss: 0.1185, Val-F2: 0.89\n",
      "[31/1000] 93s, Loss: 0.1346, Val-Loss: 0.1197, Best Val-Loss: 0.1185, Val-F2: 0.88\n",
      "[32/1000] 93s, Loss: 0.1373, Val-Loss: 0.1244, Best Val-Loss: 0.1185, Val-F2: 0.88\n",
      "[33/1000] 93s, Loss: 0.1354, Val-Loss: 0.1238, Best Val-Loss: 0.1185, Val-F2: 0.88\n",
      "[34/1000] 93s, Loss: 0.1326, Val-Loss: 0.1203, Best Val-Loss: 0.1185, Val-F2: 0.88\n",
      "[35/1000] 93s, Loss: 0.1290, Val-Loss: 0.1218, Best Val-Loss: 0.1185, Val-F2: 0.88\n",
      "[36/1000] 93s, Loss: 0.1325, Val-Loss: 0.1323, Best Val-Loss: 0.1185, Val-F2: 0.88\n",
      "[37/1000] 93s, Loss: 0.1329, Val-Loss: 0.1207, Best Val-Loss: 0.1185, Val-F2: 0.88\n",
      "New Learn Rate: 0.0001!\n",
      "[38/1000] 93s, Loss: 0.1299, Val-Loss: 0.1191, Best Val-Loss: 0.1185, Val-F2: 0.88\n",
      "Update Model!\n",
      "[39/1000] 93s, Loss: 0.1247, Val-Loss: 0.1120, Best Val-Loss: 0.1120, Val-F2: 0.89\n",
      "Update Model!\n",
      "[40/1000] 93s, Loss: 0.1223, Val-Loss: 0.1105, Best Val-Loss: 0.1105, Val-F2: 0.90\n",
      "[41/1000] 93s, Loss: 0.1219, Val-Loss: 0.1110, Best Val-Loss: 0.1105, Val-F2: 0.90\n",
      "[42/1000] 93s, Loss: 0.1194, Val-Loss: 0.1120, Best Val-Loss: 0.1105, Val-F2: 0.89\n",
      "Update Model!\n",
      "[43/1000] 93s, Loss: 0.1225, Val-Loss: 0.1100, Best Val-Loss: 0.1100, Val-F2: 0.90\n",
      "Update Model!\n",
      "[44/1000] 93s, Loss: 0.1193, Val-Loss: 0.1098, Best Val-Loss: 0.1098, Val-F2: 0.90\n",
      "[45/1000] 93s, Loss: 0.1204, Val-Loss: 0.1099, Best Val-Loss: 0.1098, Val-F2: 0.90\n",
      "[46/1000] 93s, Loss: 0.1208, Val-Loss: 0.1100, Best Val-Loss: 0.1098, Val-F2: 0.90\n",
      "[47/1000] 93s, Loss: 0.1173, Val-Loss: 0.1104, Best Val-Loss: 0.1098, Val-F2: 0.90\n",
      "Update Model!\n",
      "[48/1000] 93s, Loss: 0.1200, Val-Loss: 0.1097, Best Val-Loss: 0.1097, Val-F2: 0.90\n",
      "Update Model!\n",
      "[49/1000] 93s, Loss: 0.1179, Val-Loss: 0.1090, Best Val-Loss: 0.1090, Val-F2: 0.90\n",
      "[50/1000] 93s, Loss: 0.1176, Val-Loss: 0.1096, Best Val-Loss: 0.1090, Val-F2: 0.90\n",
      "[51/1000] 93s, Loss: 0.1163, Val-Loss: 0.1095, Best Val-Loss: 0.1090, Val-F2: 0.90\n",
      "[52/1000] 93s, Loss: 0.1182, Val-Loss: 0.1102, Best Val-Loss: 0.1090, Val-F2: 0.90\n",
      "[53/1000] 93s, Loss: 0.1173, Val-Loss: 0.1094, Best Val-Loss: 0.1090, Val-F2: 0.90\n",
      "[54/1000] 93s, Loss: 0.1150, Val-Loss: 0.1095, Best Val-Loss: 0.1090, Val-F2: 0.90\n",
      "[55/1000] 93s, Loss: 0.1168, Val-Loss: 0.1104, Best Val-Loss: 0.1090, Val-F2: 0.90\n",
      "[56/1000] 93s, Loss: 0.1178, Val-Loss: 0.1090, Best Val-Loss: 0.1090, Val-F2: 0.90\n",
      "New Learn Rate: 1e-05!\n",
      "[57/1000] 93s, Loss: 0.1164, Val-Loss: 0.1092, Best Val-Loss: 0.1090, Val-F2: 0.90\n",
      "Update Model!\n",
      "[58/1000] 93s, Loss: 0.1167, Val-Loss: 0.1087, Best Val-Loss: 0.1087, Val-F2: 0.90\n",
      "[59/1000] 93s, Loss: 0.1151, Val-Loss: 0.1094, Best Val-Loss: 0.1087, Val-F2: 0.90\n",
      "Update Model!\n",
      "[60/1000] 93s, Loss: 0.1172, Val-Loss: 0.1086, Best Val-Loss: 0.1086, Val-F2: 0.90\n",
      "Update Model!\n",
      "[61/1000] 93s, Loss: 0.1144, Val-Loss: 0.1085, Best Val-Loss: 0.1085, Val-F2: 0.90\n",
      "[62/1000] 93s, Loss: 0.1169, Val-Loss: 0.1087, Best Val-Loss: 0.1085, Val-F2: 0.90\n",
      "[63/1000] 93s, Loss: 0.1186, Val-Loss: 0.1091, Best Val-Loss: 0.1085, Val-F2: 0.90\n",
      "Update Model!\n",
      "[64/1000] 93s, Loss: 0.1155, Val-Loss: 0.1085, Best Val-Loss: 0.1085, Val-F2: 0.90\n",
      "[65/1000] 93s, Loss: 0.1168, Val-Loss: 0.1088, Best Val-Loss: 0.1085, Val-F2: 0.90\n",
      "[66/1000] 93s, Loss: 0.1138, Val-Loss: 0.1089, Best Val-Loss: 0.1085, Val-F2: 0.90\n",
      "[67/1000] 93s, Loss: 0.1126, Val-Loss: 0.1087, Best Val-Loss: 0.1085, Val-F2: 0.90\n",
      "[68/1000] 93s, Loss: 0.1164, Val-Loss: 0.1086, Best Val-Loss: 0.1085, Val-F2: 0.90\n",
      "[69/1000] 93s, Loss: 0.1173, Val-Loss: 0.1089, Best Val-Loss: 0.1085, Val-F2: 0.90\n",
      "Update Model!\n",
      "[70/1000] 93s, Loss: 0.1121, Val-Loss: 0.1084, Best Val-Loss: 0.1084, Val-F2: 0.90\n",
      "Update Model!\n",
      "[71/1000] 93s, Loss: 0.1131, Val-Loss: 0.1084, Best Val-Loss: 0.1084, Val-F2: 0.90\n",
      "[72/1000] 93s, Loss: 0.1170, Val-Loss: 0.1086, Best Val-Loss: 0.1084, Val-F2: 0.90\n",
      "Update Model!\n",
      "[73/1000] 93s, Loss: 0.1155, Val-Loss: 0.1082, Best Val-Loss: 0.1082, Val-F2: 0.90\n",
      "[74/1000] 93s, Loss: 0.1137, Val-Loss: 0.1084, Best Val-Loss: 0.1082, Val-F2: 0.90\n",
      "[75/1000] 93s, Loss: 0.1176, Val-Loss: 0.1083, Best Val-Loss: 0.1082, Val-F2: 0.90\n",
      "[76/1000] 93s, Loss: 0.1147, Val-Loss: 0.1085, Best Val-Loss: 0.1082, Val-F2: 0.90\n",
      "[77/1000] 93s, Loss: 0.1151, Val-Loss: 0.1085, Best Val-Loss: 0.1082, Val-F2: 0.90\n",
      "[78/1000] 93s, Loss: 0.1154, Val-Loss: 0.1085, Best Val-Loss: 0.1082, Val-F2: 0.90\n",
      "[79/1000] 93s, Loss: 0.1133, Val-Loss: 0.1085, Best Val-Loss: 0.1082, Val-F2: 0.90\n",
      "[80/1000] 93s, Loss: 0.1158, Val-Loss: 0.1092, Best Val-Loss: 0.1082, Val-F2: 0.90\n",
      "New Learn Rate: 1.0000000000000002e-06!\n",
      "[81/1000] 93s, Loss: 0.1133, Val-Loss: 0.1084, Best Val-Loss: 0.1082, Val-F2: 0.90\n",
      "[82/1000] 93s, Loss: 0.1148, Val-Loss: 0.1084, Best Val-Loss: 0.1082, Val-F2: 0.90\n",
      "[83/1000] 93s, Loss: 0.1174, Val-Loss: 0.1097, Best Val-Loss: 0.1082, Val-F2: 0.90\n",
      "[84/1000] 93s, Loss: 0.1161, Val-Loss: 0.1084, Best Val-Loss: 0.1082, Val-F2: 0.90\n",
      "[85/1000] 93s, Loss: 0.1153, Val-Loss: 0.1085, Best Val-Loss: 0.1082, Val-F2: 0.90\n",
      "[86/1000] 93s, Loss: 0.1143, Val-Loss: 0.1084, Best Val-Loss: 0.1082, Val-F2: 0.90\n",
      "[87/1000] 93s, Loss: 0.1181, Val-Loss: 0.1086, Best Val-Loss: 0.1082, Val-F2: 0.90\n",
      "[88/1000] 93s, Loss: 0.1163, Val-Loss: 0.1089, Best Val-Loss: 0.1082, Val-F2: 0.90\n",
      "New Learn Rate: 1.0000000000000002e-07!\n",
      "Early Stop in Epoch: 89, Best Val-Loss: 0.108198, Best F2: 0.902721\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "model = core.MyNet(17).cuda()\n",
    "# model = resnet.ResNet(resnet.BasicBlock, [2, 2, 2, 2], num_classes=17)\n",
    "criterion = nn.BCELoss(weight=None).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "estimator = core.Estimator(model, criterion, optimizer)\n",
    "for epoch in range(num_epoches):\n",
    "    time_st = time()\n",
    "    train_loader = utils.weighted_train_loader(x_tr, y_tr, probs, transform_tr, batch_size, pic_size)\n",
    "    loss_tr = estimator.train(train_loader)\n",
    "    \n",
    "    val_loader = utils.valid_loader(x_vl, y_vl, transform_vl, batch_size, pic_size)\n",
    "    loss_vl, f2_vl = estimator.validate(val_loader)\n",
    "    best_model.update(loss_vl.avg, f2_vl, estimator.model)\n",
    "    \n",
    "    if epoch > lr_tolerance and best_model.lrcount > lr_tolerance:\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "#         new_lr = 1e-3 if lr < 1e-5 else lr * 0.1\n",
    "        new_lr = lr * 0.1\n",
    "        optimizer.param_groups[0]['lr'] = new_lr\n",
    "        print('New Learn Rate: {}!'.format(new_lr))\n",
    "        best_model.lrcount = 0\n",
    "        \n",
    "    if epoch > tolerance and best_model.nobetter > tolerance:\n",
    "        best_model.save('./alexnet-709-1.pth')\n",
    "        print('Early Stop in Epoch: {}, Best Val-Loss: {:.6f}, Best F2: {:.6f}'.format(\n",
    "            epoch+1, best_model.best_loss, best_model.best_f2.value(bestf2=True)))\n",
    "        break\n",
    "        \n",
    "    print('[{}/{}] {}s, Loss: {:.4f}, Val-Loss: {:.4f}, Best Val-Loss: {:.4f}, Val-F2: {:.2f}'.format(\n",
    "        epoch+1,\n",
    "        num_epoches,\n",
    "        int(time() - time_st),\n",
    "        loss_tr.avg,\n",
    "        loss_vl.avg,\n",
    "        best_model.best_loss,\n",
    "        f2_vl.value(0.3)\n",
    "    ))\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from PIL import Image\n",
    "from time import time\n",
    "import mydataset\n",
    "import gc\n",
    "imp.reload(utils)\n",
    "imp.reload(core)\n",
    "imp.reload(mydataset)\n",
    "for i in range(10):\n",
    "    gc.collect()\n",
    "\n",
    "# 超参数\n",
    "batch_size = 256\n",
    "pic_size = (64, 64)\n",
    "learning_rate = 1e-3\n",
    "num_epoches = 1000\n",
    "tolerance = 15\n",
    "lr_tolerance = 7\n",
    "transform1 = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def transform_tr(img, pic_size):\n",
    "    img = img.resize(pic_size)\n",
    "    img = img.transpose(Image.FLIP_LEFT_RIGHT)  if random.randint(0, 1) > .5 else img\n",
    "    img = img.transpose(Image.FLIP_TOP_BOTTOM)  if random.randint(0, 1) > .5 else img\n",
    "    img.rotate(np.random.random() * 45)\n",
    "    img_tensor = transform2(img)\n",
    "    return img_tensor\n",
    "\n",
    "def transform_vl(img, pic_size):\n",
    "    img = img.resize(pic_size)\n",
    "    img_tensor = transform2(img)\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Model!\n",
      "1 [1/1000] 362s, Loss: 0.3675, Val-Loss: 0.1649, Best Val-Loss: 0.1649, Val-F2: 0.83\n",
      "Update Model!\n",
      "1 [2/1000] 184s, Loss: 0.2144, Val-Loss: 0.1512, Best Val-Loss: 0.1512, Val-F2: 0.85\n",
      "Update Model!\n",
      "1 [3/1000] 136s, Loss: 0.2001, Val-Loss: 0.1428, Best Val-Loss: 0.1428, Val-F2: 0.86\n",
      "Update Model!\n",
      "1 [4/1000] 116s, Loss: 0.1891, Val-Loss: 0.1397, Best Val-Loss: 0.1397, Val-F2: 0.86\n",
      "Update Model!\n",
      "1 [5/1000] 110s, Loss: 0.1811, Val-Loss: 0.1271, Best Val-Loss: 0.1271, Val-F2: 0.88\n",
      "1 [6/1000] 103s, Loss: 0.1758, Val-Loss: 0.1353, Best Val-Loss: 0.1271, Val-F2: 0.87\n",
      "Update Model!\n",
      "1 [7/1000] 99s, Loss: 0.1726, Val-Loss: 0.1258, Best Val-Loss: 0.1258, Val-F2: 0.88\n",
      "1 [8/1000] 97s, Loss: 0.1674, Val-Loss: 0.1281, Best Val-Loss: 0.1258, Val-F2: 0.88\n",
      "Update Model!\n",
      "1 [9/1000] 96s, Loss: 0.1666, Val-Loss: 0.1157, Best Val-Loss: 0.1157, Val-F2: 0.89\n",
      "1 [10/1000] 94s, Loss: 0.1620, Val-Loss: 0.1249, Best Val-Loss: 0.1157, Val-F2: 0.88\n",
      "1 [11/1000] 133s, Loss: 0.1592, Val-Loss: 0.1178, Best Val-Loss: 0.1157, Val-F2: 0.89\n",
      "1 [12/1000] 116s, Loss: 0.1554, Val-Loss: 0.1223, Best Val-Loss: 0.1157, Val-F2: 0.88\n",
      "Update Model!\n",
      "1 [13/1000] 108s, Loss: 0.1524, Val-Loss: 0.1106, Best Val-Loss: 0.1106, Val-F2: 0.90\n",
      "Update Model!\n",
      "1 [14/1000] 103s, Loss: 0.1509, Val-Loss: 0.1104, Best Val-Loss: 0.1104, Val-F2: 0.90\n",
      "1 [15/1000] 120s, Loss: 0.1501, Val-Loss: 0.1138, Best Val-Loss: 0.1104, Val-F2: 0.89\n",
      "Update Model!\n",
      "1 [16/1000] 119s, Loss: 0.1466, Val-Loss: 0.1047, Best Val-Loss: 0.1047, Val-F2: 0.90\n",
      "1 [17/1000] 141s, Loss: 0.1471, Val-Loss: 0.1058, Best Val-Loss: 0.1047, Val-F2: 0.90\n",
      "1 [18/1000] 160s, Loss: 0.1457, Val-Loss: 0.1061, Best Val-Loss: 0.1047, Val-F2: 0.90\n",
      "1 [19/1000] 146s, Loss: 0.1439, Val-Loss: 0.1094, Best Val-Loss: 0.1047, Val-F2: 0.90\n",
      "1 [20/1000] 145s, Loss: 0.1424, Val-Loss: 0.1115, Best Val-Loss: 0.1047, Val-F2: 0.90\n",
      "1 [21/1000] 145s, Loss: 0.1417, Val-Loss: 0.1059, Best Val-Loss: 0.1047, Val-F2: 0.90\n",
      "1 [22/1000] 182s, Loss: 0.1409, Val-Loss: 0.1103, Best Val-Loss: 0.1047, Val-F2: 0.89\n",
      "Update Model!\n",
      "1 [23/1000] 188s, Loss: 0.1378, Val-Loss: 0.1046, Best Val-Loss: 0.1046, Val-F2: 0.90\n",
      "Update Model!\n",
      "1 [24/1000] 152s, Loss: 0.1374, Val-Loss: 0.1020, Best Val-Loss: 0.1020, Val-F2: 0.91\n",
      "Update Model!\n",
      "1 [25/1000] 129s, Loss: 0.1375, Val-Loss: 0.1015, Best Val-Loss: 0.1015, Val-F2: 0.91\n",
      "Update Model!\n",
      "1 [26/1000] 118s, Loss: 0.1347, Val-Loss: 0.1008, Best Val-Loss: 0.1008, Val-F2: 0.91\n",
      "Update Model!\n",
      "1 [27/1000] 119s, Loss: 0.1341, Val-Loss: 0.0990, Best Val-Loss: 0.0990, Val-F2: 0.91\n",
      "1 [28/1000] 251s, Loss: 0.1335, Val-Loss: 0.1003, Best Val-Loss: 0.0990, Val-F2: 0.91\n",
      "1 [29/1000] 180s, Loss: 0.1323, Val-Loss: 0.1069, Best Val-Loss: 0.0990, Val-F2: 0.90\n",
      "1 [30/1000] 164s, Loss: 0.1313, Val-Loss: 0.1011, Best Val-Loss: 0.0990, Val-F2: 0.91\n",
      "1 [31/1000] 292s, Loss: 0.1298, Val-Loss: 0.1015, Best Val-Loss: 0.0990, Val-F2: 0.91\n",
      "1 [32/1000] 217s, Loss: 0.1296, Val-Loss: 0.1038, Best Val-Loss: 0.0990, Val-F2: 0.91\n",
      "1 [33/1000] 212s, Loss: 0.1278, Val-Loss: 0.1000, Best Val-Loss: 0.0990, Val-F2: 0.91\n",
      "1 [34/1000] 205s, Loss: 0.1251, Val-Loss: 0.1033, Best Val-Loss: 0.0990, Val-F2: 0.90\n",
      "New Learn Rate: 0.0001!\n",
      "1 [35/1000] 232s, Loss: 0.1244, Val-Loss: 0.1005, Best Val-Loss: 0.0990, Val-F2: 0.91\n",
      "Update Model!\n",
      "1 [36/1000] 199s, Loss: 0.1208, Val-Loss: 0.0960, Best Val-Loss: 0.0960, Val-F2: 0.92\n",
      "Update Model!\n",
      "1 [37/1000] 315s, Loss: 0.1202, Val-Loss: 0.0954, Best Val-Loss: 0.0954, Val-F2: 0.92\n",
      "Update Model!\n",
      "1 [38/1000] 212s, Loss: 0.1175, Val-Loss: 0.0949, Best Val-Loss: 0.0949, Val-F2: 0.92\n",
      "1 [39/1000] 189s, Loss: 0.1176, Val-Loss: 0.0954, Best Val-Loss: 0.0949, Val-F2: 0.92\n",
      "1 [40/1000] 204s, Loss: 0.1167, Val-Loss: 0.0956, Best Val-Loss: 0.0949, Val-F2: 0.92\n",
      "1 [41/1000] 215s, Loss: 0.1176, Val-Loss: 0.0959, Best Val-Loss: 0.0949, Val-F2: 0.91\n",
      "1 [42/1000] 216s, Loss: 0.1155, Val-Loss: 0.0954, Best Val-Loss: 0.0949, Val-F2: 0.91\n",
      "Update Model!\n",
      "1 [43/1000] 262s, Loss: 0.1152, Val-Loss: 0.0949, Best Val-Loss: 0.0949, Val-F2: 0.92\n",
      "1 [44/1000] 296s, Loss: 0.1150, Val-Loss: 0.0965, Best Val-Loss: 0.0949, Val-F2: 0.91\n",
      "1 [45/1000] 322s, Loss: 0.1150, Val-Loss: 0.0953, Best Val-Loss: 0.0949, Val-F2: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [46/1000] 234s, Loss: 0.1135, Val-Loss: 0.0965, Best Val-Loss: 0.0949, Val-F2: 0.91\n",
      "1 [47/1000] 290s, Loss: 0.1142, Val-Loss: 0.0968, Best Val-Loss: 0.0949, Val-F2: 0.91\n",
      "1 [48/1000] 350s, Loss: 0.1127, Val-Loss: 0.0968, Best Val-Loss: 0.0949, Val-F2: 0.91\n",
      "1 [49/1000] 367s, Loss: 0.1131, Val-Loss: 0.0962, Best Val-Loss: 0.0949, Val-F2: 0.91\n",
      "1 [50/1000] 348s, Loss: 0.1137, Val-Loss: 0.0955, Best Val-Loss: 0.0949, Val-F2: 0.91\n",
      "New Learn Rate: 1e-05!\n",
      "1 [51/1000] 313s, Loss: 0.1133, Val-Loss: 0.0949, Best Val-Loss: 0.0949, Val-F2: 0.92\n",
      "1 [52/1000] 285s, Loss: 0.1142, Val-Loss: 0.0954, Best Val-Loss: 0.0949, Val-F2: 0.91\n",
      "1 [53/1000] 319s, Loss: 0.1123, Val-Loss: 0.0954, Best Val-Loss: 0.0949, Val-F2: 0.91\n",
      "1 [54/1000] 235s, Loss: 0.1112, Val-Loss: 0.0954, Best Val-Loss: 0.0949, Val-F2: 0.92\n",
      "1 [55/1000] 159s, Loss: 0.1118, Val-Loss: 0.0954, Best Val-Loss: 0.0949, Val-F2: 0.91\n",
      "1 [56/1000] 149s, Loss: 0.1118, Val-Loss: 0.0956, Best Val-Loss: 0.0949, Val-F2: 0.92\n",
      "1 [57/1000] 153s, Loss: 0.1125, Val-Loss: 0.0954, Best Val-Loss: 0.0949, Val-F2: 0.92\n",
      "1 [58/1000] 148s, Loss: 0.1126, Val-Loss: 0.0960, Best Val-Loss: 0.0949, Val-F2: 0.91\n",
      "New Learn Rate: 1.0000000000000002e-06!\n",
      "Early Stop in Epoch: 59, Best Val-Loss: 0.094861, Best F2: 0.918830\n",
      "Update Model!\n",
      "2 [1/1000] 381s, Loss: 0.3769, Val-Loss: 0.1749, Best Val-Loss: 0.1749, Val-F2: 0.82\n",
      "Update Model!\n",
      "2 [2/1000] 203s, Loss: 0.2169, Val-Loss: 0.1609, Best Val-Loss: 0.1609, Val-F2: 0.84\n",
      "Update Model!\n",
      "2 [3/1000] 192s, Loss: 0.1980, Val-Loss: 0.1490, Best Val-Loss: 0.1490, Val-F2: 0.85\n",
      "2 [4/1000] 194s, Loss: 0.1896, Val-Loss: 0.1498, Best Val-Loss: 0.1490, Val-F2: 0.85\n",
      "Update Model!\n",
      "2 [5/1000] 184s, Loss: 0.1824, Val-Loss: 0.1317, Best Val-Loss: 0.1317, Val-F2: 0.88\n",
      "Update Model!\n",
      "2 [6/1000] 153s, Loss: 0.1779, Val-Loss: 0.1314, Best Val-Loss: 0.1314, Val-F2: 0.87\n",
      "2 [7/1000] 128s, Loss: 0.1726, Val-Loss: 0.1353, Best Val-Loss: 0.1314, Val-F2: 0.86\n",
      "Update Model!\n",
      "2 [8/1000] 138s, Loss: 0.1699, Val-Loss: 0.1199, Best Val-Loss: 0.1199, Val-F2: 0.88\n",
      "Update Model!\n",
      "2 [9/1000] 152s, Loss: 0.1664, Val-Loss: 0.1188, Best Val-Loss: 0.1188, Val-F2: 0.89\n",
      "2 [10/1000] 148s, Loss: 0.1627, Val-Loss: 0.1203, Best Val-Loss: 0.1188, Val-F2: 0.88\n",
      "Update Model!\n",
      "2 [11/1000] 207s, Loss: 0.1604, Val-Loss: 0.1168, Best Val-Loss: 0.1168, Val-F2: 0.89\n",
      "Update Model!\n",
      "2 [12/1000] 186s, Loss: 0.1596, Val-Loss: 0.1149, Best Val-Loss: 0.1149, Val-F2: 0.90\n",
      "Update Model!\n",
      "2 [13/1000] 183s, Loss: 0.1542, Val-Loss: 0.1133, Best Val-Loss: 0.1133, Val-F2: 0.89\n",
      "Update Model!\n",
      "2 [14/1000] 186s, Loss: 0.1537, Val-Loss: 0.1083, Best Val-Loss: 0.1083, Val-F2: 0.90\n",
      "2 [15/1000] 207s, Loss: 0.1525, Val-Loss: 0.1147, Best Val-Loss: 0.1083, Val-F2: 0.89\n",
      "2 [16/1000] 209s, Loss: 0.1496, Val-Loss: 0.1088, Best Val-Loss: 0.1083, Val-F2: 0.90\n",
      "Update Model!\n",
      "2 [17/1000] 209s, Loss: 0.1482, Val-Loss: 0.1074, Best Val-Loss: 0.1074, Val-F2: 0.90\n",
      "2 [18/1000] 388s, Loss: 0.1469, Val-Loss: 0.1117, Best Val-Loss: 0.1074, Val-F2: 0.90\n",
      "Update Model!\n",
      "2 [19/1000] 379s, Loss: 0.1437, Val-Loss: 0.1072, Best Val-Loss: 0.1072, Val-F2: 0.90\n",
      "Update Model!\n",
      "2 [20/1000] 438s, Loss: 0.1431, Val-Loss: 0.1044, Best Val-Loss: 0.1044, Val-F2: 0.90\n",
      "2 [21/1000] 500s, Loss: 0.1416, Val-Loss: 0.1064, Best Val-Loss: 0.1044, Val-F2: 0.90\n",
      "Update Model!\n",
      "2 [22/1000] 383s, Loss: 0.1389, Val-Loss: 0.1017, Best Val-Loss: 0.1017, Val-F2: 0.91\n",
      "2 [23/1000] 387s, Loss: 0.1383, Val-Loss: 0.1066, Best Val-Loss: 0.1017, Val-F2: 0.90\n",
      "2 [24/1000] 203s, Loss: 0.1391, Val-Loss: 0.1072, Best Val-Loss: 0.1017, Val-F2: 0.90\n",
      "2 [25/1000] 147s, Loss: 0.1368, Val-Loss: 0.1054, Best Val-Loss: 0.1017, Val-F2: 0.90\n",
      "Update Model!\n",
      "2 [26/1000] 123s, Loss: 0.1361, Val-Loss: 0.0991, Best Val-Loss: 0.0991, Val-F2: 0.91\n",
      "2 [27/1000] 112s, Loss: 0.1344, Val-Loss: 0.1057, Best Val-Loss: 0.0991, Val-F2: 0.90\n",
      "2 [28/1000] 107s, Loss: 0.1329, Val-Loss: 0.1020, Best Val-Loss: 0.0991, Val-F2: 0.91\n",
      "2 [29/1000] 104s, Loss: 0.1310, Val-Loss: 0.1013, Best Val-Loss: 0.0991, Val-F2: 0.91\n",
      "2 [30/1000] 103s, Loss: 0.1320, Val-Loss: 0.1045, Best Val-Loss: 0.0991, Val-F2: 0.90\n",
      "2 [31/1000] 108s, Loss: 0.1303, Val-Loss: 0.1035, Best Val-Loss: 0.0991, Val-F2: 0.91\n",
      "2 [32/1000] 104s, Loss: 0.1297, Val-Loss: 0.1017, Best Val-Loss: 0.0991, Val-F2: 0.91\n",
      "2 [33/1000] 103s, Loss: 0.1288, Val-Loss: 0.1022, Best Val-Loss: 0.0991, Val-F2: 0.91\n",
      "New Learn Rate: 0.0001!\n",
      "2 [34/1000] 113s, Loss: 0.1281, Val-Loss: 0.1046, Best Val-Loss: 0.0991, Val-F2: 0.90\n",
      "Update Model!\n",
      "2 [35/1000] 164s, Loss: 0.1231, Val-Loss: 0.0969, Best Val-Loss: 0.0969, Val-F2: 0.91\n",
      "Update Model!\n",
      "2 [36/1000] 161s, Loss: 0.1202, Val-Loss: 0.0963, Best Val-Loss: 0.0963, Val-F2: 0.91\n",
      "Update Model!\n",
      "2 [37/1000] 166s, Loss: 0.1189, Val-Loss: 0.0959, Best Val-Loss: 0.0959, Val-F2: 0.91\n",
      "Update Model!\n",
      "2 [38/1000] 156s, Loss: 0.1189, Val-Loss: 0.0955, Best Val-Loss: 0.0955, Val-F2: 0.91\n",
      "2 [39/1000] 165s, Loss: 0.1187, Val-Loss: 0.0963, Best Val-Loss: 0.0955, Val-F2: 0.91\n",
      "2 [40/1000] 197s, Loss: 0.1190, Val-Loss: 0.0961, Best Val-Loss: 0.0955, Val-F2: 0.91\n",
      "2 [41/1000] 188s, Loss: 0.1177, Val-Loss: 0.0963, Best Val-Loss: 0.0955, Val-F2: 0.91\n",
      "2 [42/1000] 1672s, Loss: 0.1170, Val-Loss: 0.0966, Best Val-Loss: 0.0955, Val-F2: 0.91\n",
      "2 [43/1000] 291s, Loss: 0.1175, Val-Loss: 0.0960, Best Val-Loss: 0.0955, Val-F2: 0.91\n",
      "2 [44/1000] 190s, Loss: 0.1177, Val-Loss: 0.0955, Best Val-Loss: 0.0955, Val-F2: 0.91\n",
      "2 [45/1000] 223s, Loss: 0.1162, Val-Loss: 0.0960, Best Val-Loss: 0.0955, Val-F2: 0.91\n",
      "New Learn Rate: 1e-05!\n",
      "2 [46/1000] 533s, Loss: 0.1171, Val-Loss: 0.0962, Best Val-Loss: 0.0955, Val-F2: 0.91\n",
      "Update Model!\n",
      "2 [47/1000] 389s, Loss: 0.1155, Val-Loss: 0.0953, Best Val-Loss: 0.0953, Val-F2: 0.92\n",
      "2 [48/1000] 218s, Loss: 0.1150, Val-Loss: 0.0954, Best Val-Loss: 0.0953, Val-F2: 0.92\n",
      "2 [49/1000] 165s, Loss: 0.1152, Val-Loss: 0.0960, Best Val-Loss: 0.0953, Val-F2: 0.91\n",
      "2 [50/1000] 273s, Loss: 0.1150, Val-Loss: 0.0963, Best Val-Loss: 0.0953, Val-F2: 0.91\n",
      "2 [51/1000] 569s, Loss: 0.1148, Val-Loss: 0.0958, Best Val-Loss: 0.0953, Val-F2: 0.91\n",
      "2 [52/1000] 440s, Loss: 0.1147, Val-Loss: 0.0955, Best Val-Loss: 0.0953, Val-F2: 0.91\n",
      "2 [53/1000] 432s, Loss: 0.1164, Val-Loss: 0.0960, Best Val-Loss: 0.0953, Val-F2: 0.91\n",
      "2 [54/1000] 399s, Loss: 0.1136, Val-Loss: 0.0962, Best Val-Loss: 0.0953, Val-F2: 0.91\n",
      "New Learn Rate: 1.0000000000000002e-06!\n",
      "2 [55/1000] 522s, Loss: 0.1138, Val-Loss: 0.0957, Best Val-Loss: 0.0953, Val-F2: 0.92\n",
      "2 [56/1000] 363s, Loss: 0.1146, Val-Loss: 0.0957, Best Val-Loss: 0.0953, Val-F2: 0.91\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "model_name = 'weightedNN2'\n",
    "date = '711'\n",
    "probs1 = [0.1, 0.2, 0.4, 0.4, 0.1, 0.1, 0.2, 0.2, 0.4, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "probs = [0.5, 0.6, 0.8, 0.8, 0.5, 0.5, 0.6, 0.6, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import islice\n",
    "file_all = train_set['image_name'].values\n",
    "test_file_all = test_set['image_name'].values\n",
    "y_all = utils.get_y(train_set['tags'].values, label_map)\n",
    "pred_tr = np.zeros((file_all.shape[0], 17))\n",
    "pred_ts = np.zeros((test_file_all.shape[0], 17))\n",
    "kf = KFold(n_splits = n_splits)\n",
    "\n",
    "k_now = 0\n",
    "for i_tr, i_vl in islice(kf.split(y_all), 0, None):\n",
    "    model = core.MyNet(17).cuda()\n",
    "    criterion = nn.BCELoss(weight=None).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    estimator = core.Estimator(model, criterion, optimizer)\n",
    "    best_model = core.BestModel()\n",
    "    \n",
    "    for epoch in range(num_epoches):\n",
    "        time_st = time()\n",
    "        # 训练\n",
    "        train_loader = utils.weighted_train_loader(file_all[i_tr], y_all[i_tr], probs, transform_tr, batch_size, pic_size)\n",
    "        loss_tr = estimator.train(train_loader)\n",
    "\n",
    "        # 验证\n",
    "        val_loader = utils.valid_loader(file_all[i_vl], y_all[i_vl], transform_vl, batch_size, pic_size)\n",
    "        loss_vl, f2_vl = estimator.validate(val_loader)\n",
    "        best_model.update(loss_vl.avg, f2_vl, estimator.model)\n",
    "        \n",
    "        # 若验证结果提升缓慢，减小学习率\n",
    "        if epoch > lr_tolerance and best_model.lrcount > lr_tolerance:\n",
    "            optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * 0.1\n",
    "            print('New Learn Rate: {}!'.format(optimizer.param_groups[0]['lr']))\n",
    "            best_model.lrcount = 0\n",
    "        \n",
    "        # 若验证结果不再提升，保存模型、验证结果、预测结果，跳出迭代\n",
    "        if epoch > tolerance and best_model.nobetter > tolerance:\n",
    "            print('Early Stop in Epoch: {}, Best Val-Loss: {:.6f}, Best F2: {:.6f}'.format(\n",
    "                epoch+1, best_model.best_loss, best_model.best_f2.value(bestf2=True)))\n",
    "            pred_tr[i_vl, :] = f2_vl.preds\n",
    "            best_model.save('./model/model{}-date{}-kf{}.pth'.format(model_name, date, k_now+1))\n",
    "                                                               \n",
    "            tst_loader = utils.test_loader(test_file_all, transform_vl, batch_size, pic_size)\n",
    "            estimator.model.load_state_dict(best_model.best_model)                                       \n",
    "            pred_ts_temp = estimator.predict(tst_loader)\n",
    "            pred_ts =  pred_ts_temp / float(n_splits)\n",
    "            break\n",
    "        \n",
    "        # 打印每一次迭代的训练验证成绩\n",
    "        print('{} [{}/{}] {}s, Loss: {:.4f}, Val-Loss: {:.4f}, Best Val-Loss: {:.4f}, Val-F2: {:.2f}'.format(\n",
    "            k_now+1, epoch+1, num_epoches, int(time() - time_st), loss_tr.avg, loss_vl.avg, \n",
    "            best_model.best_loss, f2_vl.value(0.3)))\n",
    "        gc.collect()\n",
    "    k_now += 1\n",
    "\n",
    "# 序列化验证和预测结果，用于stacking\n",
    "np.save('./pred/model{}_date{}_pred_train.npy'.format(model_name, date), pred_tr)\n",
    "np.save('./pred/model{}_date{}_pred_test.npy'.format(model_name, date), pred_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(core)\n",
    "imp.reload(utils)\n",
    "imp.reload(mydataset)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import islice\n",
    "file_all = train_set['image_name'].values\n",
    "test_file_all = test_set['image_name'].values\n",
    "y_all = utils.get_y(train_set['tags'].values, label_map)\n",
    "kf = KFold(n_splits = 5)\n",
    "\n",
    "pred_tr = np.zeros((file_all.shape[0], 17))\n",
    "pred_ts = np.zeros((test_file_all.shape[0], 17))\n",
    "model = core.MyNet(17).cuda()\n",
    "criterion = nn.BCELoss(weight=None).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "estimator = core.Estimator(model, criterion, optimizer)\n",
    "k_now = 1\n",
    "for i_tr, i_vl in islice(kf.split(y_all), 0, None):\n",
    "    best_model = core.BestModel()\n",
    "    estimator.model.load_state_dict(torch.load('modelweightedNN1-date710-kf{}.pth'.format(k_now)))\n",
    "    val_loader = utils.valid_loader(file_all[i_vl], y_all[i_vl], transform_vl, batch_size, pic_size)\n",
    "    loss_vl, f2_vl = estimator.validate(val_loader)\n",
    "    pred_tr[i_vl] = f2_vl.preds\n",
    "    \n",
    "    tst_loader = utils.test_loader(test_file_all, transform_vl, batch_size, pic_size)\n",
    "    pred_ts += estimator.predict(tst_loader) / 5.0\n",
    "    k_now += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92196563004229037"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th1 = utils.f2_opti_score(y_all, pred_tr, thresholds = np.arange(0, 1, 0.01), num_classes=17)\n",
    "th2 = utils.f2_opti_score(y_all, pred_tr, thresholds = np.arange(1, 0, -0.01), num_classes=17)\n",
    "th = (th1 + th2) / 2.0\n",
    "print(utils.f2_score(y_all, pred_tr, th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp.reload(utils)\n",
    "submit_df = utils.to_submit(pred_ts, th, test_set, inv_label_map)\n",
    "submit_df.to_csv('./submit/model{}_date{}_no{}.csv'.format(model_name, date, 1), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'weightedNN1'\n",
    "date = '710'\n",
    "np.save('./pred/model{}_date{}_pred_train.npy'.format(model_name, date), pred_tr)\n",
    "np.save('./pred/model{}_date{}_pred_test.npy'.format(model_name, date), pred_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
